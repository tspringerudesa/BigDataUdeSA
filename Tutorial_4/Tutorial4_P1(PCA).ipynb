{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnMR3sOW6j5y"
   },
   "source": [
    "# Tutorial de Big Data (UdeSA) 2025\n",
    "## Tutorial 4 - Parte 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFE_dv7D75Da"
   },
   "source": [
    "Componentes principales (PCA, en inglés) es una técnica de **aprendizaje no supervisado**. Es decir que nos encontramos en una situación donde tenemos información de un conjunto de variables o features ($X_1, X_2, ..., X_p$), pero no sobre una variable de resultado o outcome ($Y$). Vamos a tratar de ajustar algoritmos que interpreten la distribución de nuestros datos y encuentren relaciones interesantes entre éstos, trabajando con la naturaleza propia de los datos y sin un outcome de interés $Y$.  \n",
    "Esto se diferencia del **aprendizaje supervisado**, caso en el cual los estimadores se usan para **predecir** resultados basados en datos que poseen un outcome o variable de resultado $Y$ (puede ser una etiqueta -clasificación- o un valor -regresión-).\n",
    "\n",
    "Los algoritmos de aprendizaje no supervisado pueden ser muy útiles para casos en los que se busca **reducir la dimensionalidad**, por ejemplo cuando se busca visualizar datos de gran dimensionalidad o se busca crear un índice. PCA suele emplearse como parte del **análisis descriptivo y exploratorio de datos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que tenemos $n$ observaciones y $p$ variables y queremos visualizarlas como parte de una análisis exploratorio de los datos.\n",
    "\n",
    "Podríamos realizar gráficos de a 2 variables, pero serían muchos si $p$ es grande... $$ {p \\choose 2} = \\frac{p!}{2(p-2)!}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Range of n values\n",
    "n_values = np.arange(0, 21)  # from 0 to 20\n",
    "combinations = n_values * (n_values - 1) / 2\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(n_values, combinations, marker='o', color='#f9b2d0')\n",
    "plt.title(r'$\\binom{n}{2}$ vs. $n$')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel(r'$\\binom{n}{2}$')\n",
    "plt.grid(False)\n",
    "plt.xticks(n_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Entonces vamos a buscar una representación de los datos en menos dimensiones (2 usualmente) que capture la mayor información posible.\n",
    "\n",
    "Las dimensiones serán combinaciones lineales de las $p$ variables que tienen la mayor varianza posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWiCdrSF_fqo"
   },
   "source": [
    "Vamos a trabajar con la librería [Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statsmodels\n",
    "#!pip install scikit-learn\n",
    "#!pip install ISLP\n",
    "\n",
    "import ISLP\n",
    "from ISLP import load_data\n",
    "from statsmodels.datasets import get_rdataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con una base de datos que provee el libro ISLP.\n",
    "\n",
    "##### Violent Crime Rates by US State\n",
    "Contiene información sobre arrestos por asaltos, asesinatos y violaciones cada 100.000 habitantes en 50 estados de Estados Unidos en 1973. También tiene información sobre el porcentaje de población viviendo en zonas urbanas.\n",
    "\n",
    "- Murder: Murder arrests (per 100,000)\n",
    "- Assault: Assault arrests (per 100,000)\n",
    "- Rape: Rape arrests (per 100,000)\n",
    "- UrbanPop: Percent urban population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests = get_rdataset('USArrests').data\n",
    "print(arrests.shape)\n",
    "print(\"\\n\", arrests.info)\n",
    "print(\"\\n\", arrests.dtypes)\n",
    "print(\"\\n\", arrests.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arrests.mean())\n",
    "\n",
    "# Escalamos las variables\n",
    "# Inicializamos el transformador\n",
    "scaler = StandardScaler(with_std=True, with_mean=True)\n",
    "# Aplicamos fit_transform al DataFrame\n",
    "arrests_transformed = pd.DataFrame(scaler.fit_transform(arrests), columns=arrests.columns)\n",
    "print(arrests_transformed.mean()) # luego de la estandarización la media es cero\n",
    "#print(arrests_transformed.std()) # la desviación estandar es uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Por defecto, PCA() centra las variables para que tengan media cero pero no las escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos PCA. Estamos buscando maximizar la varianza de los predictores con la restricción de normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el modelo\n",
    "pca = PCA()\n",
    "arrests_pca = pca.fit_transform(arrests_transformed)\n",
    "\n",
    "# Scores\n",
    "scores = arrests_pca\n",
    "\n",
    "# % de la Varianza explicada por los componentes \n",
    "print(\"Varianza explicada:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Loadings vectors\n",
    "loading_vectors = pca.components_ # cada fila corresponde a un CP y cada columna, a una variable\n",
    "print(\"Loadings:\\n\", pca.components_)\n",
    "print(\"Loadings del CP1:\\n\",pca.components_[0]) \n",
    "pca.components_[0,0] #loadings del CP1 variable 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notar que si tomamos los loadings del primer componente principal, por ejemplo:\n",
    "(0.53589947)**2+(0.58318363)**2+(0.27819087)**2+(0.5434309)**2 \n",
    "# La suma de sus cuadrados vemos que es igual a 1. Es la restricción que habíamos puesto!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot\n",
    "i, j = 0, 1 # Componentes\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8)) # creamos 1 subplot\n",
    "ax.scatter(scores[:,0], scores[:,1]) # graficamos los valores de los CP1 y CP2\n",
    "ax.set_xlabel('CP%d' % (i+1))\n",
    "ax.set_ylabel('CP%d' % (j+1))\n",
    "for k in range(pca.components_.shape[1]): # loop que itera por la cantidad de features\n",
    "    ax.arrow(0, 0, pca.components_[i,k], pca.components_[j,k]) # flecha desde el origen (0) a las coordenadas\n",
    "    ax.text(pca.components_[i,k], pca.components_[j,k], arrests.columns[k]) # al final de cada flecha, nombre de la variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biplot\n",
    "# Ajustes, extendemos longitud de las flechas e invertimos el eje y\n",
    "\n",
    "i, j = 0, 1 # Componentes\n",
    "\n",
    "scale_arrow = s_ = 2 # para extender la longitud de las flechas y que se vean mejor\n",
    "scores[:,1] *= -1\n",
    "pca.components_[1] *= -1 # gira el eje y (CP2)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.scatter(scores[:,0], scores[:,1]) \n",
    "ax.set_xlabel('CP%d' % (i+1))\n",
    "ax.set_ylabel('CP%d' % (j+1))\n",
    "for k in range(pca.components_.shape[1]):\n",
    "    ax.arrow(0, 0, s_*pca.components_[i,k], s_*pca.components_[j,k])\n",
    "    ax.text(s_*pca.components_[i,k], s_*pca.components_[j,k], arrests.columns[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % de la Varianza explicada por los componentes \n",
    "print(pca.explained_variance_ratio_) # CP1 explica el 62% de la varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6)) # 2 subplots uno al lado del otro\n",
    "ticks = np.arange(pca.n_components_)+1 # para crear ticks en el eje horizontal\n",
    "ax = axes[0]\n",
    "ax.plot(ticks, pca.explained_variance_ratio_ , marker='o')\n",
    "ax.set_xlabel('Componente principal');\n",
    "ax.set_ylabel('Proporción de la varianza explicada por cada componente')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xticks(ticks)\n",
    "# capture suprime la visualización de la figura parcialmente terminada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = axes[1]\n",
    "ax.plot(ticks, pca.explained_variance_ratio_.cumsum(), marker='o') \n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Suma acumulada de la varianza explicada')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xticks(ticks)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar con un dataset de vinos (de scikit-learn). Contiene características de 178 vinos y a qué segmento de consumidores pertenecen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el dataset y breve exploración\n",
    "wine_data = pd.read_csv('/Users/tomaspacheco/Documents/GitHub/BigDataUdeSA/Tutorial_4/Wine.csv')\n",
    "print(wine_data.shape)\n",
    "print(wine_data.dtypes)\n",
    "print(wine_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos datos entre X e Y (por ahora, haremos de cuenta que no contamos con Y)\n",
    "wine_features = wine_data.iloc[:, 0:13].values\n",
    "wine_customer_segment = wine_data.iloc[:, 13].values\n",
    "\n",
    "# Vemos las etiquetas posibles de customer segment\n",
    "wine_customer_segment_unique, counts = np.unique(wine_customer_segment, return_counts=True)\n",
    "for value, count in zip(wine_customer_segment_unique, counts):\n",
    "    print(f\"Value: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento. Estandarizar las variables\n",
    "# Iniciar scaler y aplicarlo\n",
    "sc = StandardScaler()\n",
    "wine_features_transformed = sc.fit_transform(wine_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por qué estandarizamos? El análisis es sensible a la varianza de las variables originales y eso puede ocasionar problemas a la hora de elegir los CPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar PCA\n",
    "pca = PCA(n_components = 2)\n",
    "wine_pca = pca.fit_transform(wine_features_transformed) # Obtenemos los scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "wine_scores = wine_pca\n",
    "\n",
    "# % de la Varianza explicada por los componentes\n",
    "print(\"Varianza explicada:\", pca.explained_variance_ratio_)\n",
    "# El primer componente principal explica el 36% de la varianza, mientras que el segundo, explica el 19%\n",
    "\n",
    "# Loading vectors\n",
    "loading_vectors = pca.components_ # cada fila corresponde a un CP y cada columna, a una variable\n",
    "print(\"Loadings:\\n\", pca.components_)\n",
    "print(\"Loadings del CP1:\\n\",pca.components_[0]) \n",
    "\n",
    "# Visualizamos features y loadings\n",
    "for i, loading_vector in enumerate(loading_vectors):\n",
    "    print(f\"\\nLoading Vector CP{i+1}:\")\n",
    "    for j, feature in enumerate(wine_data.columns[:-1]):\n",
    "        print(f\"{feature}: {round(loading_vector[j],3)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame para los componentes principales\n",
    "pca_df = pd.DataFrame(data=wine_pca, columns=['Componente_1', 'Componente_2'])\n",
    "\n",
    "# Añadir la variable objetivo al DataFrame de los componentes principales\n",
    "pca_df['Customer_Segment'] = wine_customer_segment\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los componentes\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_df['Componente_1'], pca_df['Componente_2'], c = wine_data['Customer_Segment'], cmap='viridis')\n",
    "plt.xlabel('Principal Component 1', fontsize=11)\n",
    "plt.ylabel('Principal Component 2', fontsize=11)\n",
    "plt.title('PCA - Components 1 and 2')\n",
    "plt.colorbar(label='Customer Segment')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Otra forma de graficar\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.xlabel('Principal Component 1',fontsize=12)\n",
    "plt.ylabel('Principal Component 2',fontsize=12)\n",
    "plt.title(\"Wine Data\",fontsize=16)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "targets = [1, 2, 3]\n",
    "colors = ['#b11a46', '#ac9a94', '#692a40']\n",
    "\n",
    "for target, color in zip(targets,colors):\n",
    "    indices_graf = pca_df['Customer_Segment'] == target\n",
    "    plt.scatter(pca_df.loc[indices_graf, 'Componente_1'], pca_df.loc[indices_graf, 'Componente_2'], c = color, s = 50)\n",
    "\n",
    "#plt.xlim(-4,4)\n",
    "#plt.ylim(-4,4)\n",
    "plt.legend(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La representación bidimensional de los datos tridimensionales capta correctamente el patrón principal de los datos: las observaciones rojas, azules y verdes, siguen estando en la representación bidimensional. \n",
    "\n",
    "Nota: aquí usamos dos componentes pero podríamos haber usado 1 o más de 2. Para decidir qué número de componentes usar, podemos consultar un scree plot que nos muestre la proporción de variable explicada para cada uno de los componentes y la variación en la varianza total explicada por el total de los componentes.\n",
    "\n",
    "Típicamente se elige la cantidad de componentes para la cual la proporción de la varianza explicada cae para cada componente principal adicional (cuando hay un codo en el scree plot)\n",
    "\n",
    "Hagamos este grafico!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA con todos los componentes\n",
    "\n",
    "pca_all = PCA()\n",
    "wine_pca2 = pca_all.fit_transform(wine_features_transformed) # Obtenemos los scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6)) # 2 subplots uno al lado del otro\n",
    "ticks = np.arange(pca_all.n_components_)+1 # para crear ticks en el eje horizontal\n",
    "ax = axes[0]\n",
    "ax.plot(ticks, pca_all.explained_variance_ratio_ , marker='o', c = '#268e40')\n",
    "ax.set_xlabel('Componente principal');\n",
    "ax.set_ylabel('Proporción de la varianza explicada por cada componente')\n",
    "ax.set_ylim([0,1.05])\n",
    "ax.set_xticks(ticks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = axes[1]\n",
    "ax.plot(ticks, pca_all.explained_variance_ratio_.cumsum(), marker='o', c = '#0ca3db') \n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Suma acumulada de la varianza explicada')\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.set_xticks(ticks)\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "NFyrvNVL26HH",
    "pEiQnszy3lmi",
    "1yaJI88z4TfW",
    "ZoQVkU4C-GNd"
   ],
   "name": "Aprendizaje_no_supervisado.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "988c801e8fa6188d3e53012a7256361dd6100dad47899d4700f624e035bcb20b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
